---
title: Exam Aitana Egea
output: html_document
---

EXERCISE 1. 


```{r cars}
library(dplyr)
library(tidyr)
library(tibble)
library(data.table)
library(magrittr)
library(stringr)
library(IRanges)
library(Biostrings)
library(GenomicRanges)
library(biomaRt)
library(ggplot2)

```
Your goal is to simulate the final reads you get in the experiment when you sequence the starting DNA with 20x coverage. The sequencing technology that you are using creates reads of 100bp +/- 6bp length (avg +/- sd, normally distributed). Any given base that you read has 0.3% chance of being read incorrectly. Reads are being made starting from random positions within the genome. Base coverage is defined as how many times a given position (base) in your starting sequence is present in the reads, on average. 

```{r cars}
dna_seq <- readDNAStringSet("ftp://ftp.sanger.ac.uk/pub/project/pathogens/sa/MRSA252.dna") 

seq <- dna_seq[[1]][1:35000]


my.kmer.coverage <- function(dna, kmers, cov) {

     
    length_genome <- nchar(dna) # get the genome length
    read_start <- sample(length_genome-107, (length_genome/100)*cov, replace = T) # select read start position randomly
    read_lengths <-  round(rnorm((length_genome%/%100)*cov, 100, 6)) #create the lengths following the indicated instructions
    
h#Extract the reads from the sequence
    irextract <- IRanges(start = read_start, width = read_lengths)
    reads <- extractAt(dna, irextract)
    
#INTRODUCING THE RANDOM ERRORS: Introduce the errors randomly (at random positions). I also created a vector of random bases
#However, if I just replace them at the random starting positions, there's a chance that the 
#existing base is the same as the new one, and this would diminish the error rate considerably. 
#Therefore, I had to modify the vector of new letters, comparing each letter with the "Old" letters
#at each random position. I modified the ones that are equal, and then replaced the letters at random 
#positions. 
    
    
    perfect_reads <- unlist(reads)
    
    err_pos <- sample(nchar(perfect_reads),nchar(perfect_reads)*(0.3/100), replace=TRUE)
    
    letter_sample <- DNAString( paste(sample(c('A', 'C', 'G', 'T'), length(err_pos),  replace=TRUE), collapse=""))
    
    correct_letters <- perfect_reads[err_pos]
    
    cmp <- compareStrings(letter_sample, correct_letters)
    
    positions_A <- unlist(str_locate_all(cmp, "A"))
    
    positions_TGC <- unlist(str_locate_all(cmp, "[TCG]"))
    
    new <- letter_sample
    
    new[positions_A] <- "G"
    
    new[positions_TGC] <- "A"
    
    
    perfect_reads_mismatch <- replaceLetterAt(perfect_reads, err_pos, new)
    
    eFrags <- DNAStringSet(perfect_reads_mismatch, 
        start=c(1, (cumsum(width(reads))+1)[-length(reads)]), 
        width=width(reads))


#OBTAINING THE K-MERS: I unlisted the reads with errors, and created an Iranges
#that contains the ranges of each read. Then I created a Granges object with identifier
#for each read, that creates a sliding window by identifier. I unlisted the windows, and then
#extracted the sequences from the inlisted fragments. 
    
   unlisted <- unlist(eFrags)
   reads_width <- width(eFrags)
   ranges <- successiveIRanges(reads_width, gapwidth=0, from=1)
   dt <- as.data.table(ranges)
   dt <- dt %>% mutate(seqnames = row_number())
   gr <- GRanges(dt)
   windows = slidingWindows(gr, width=kmers, step=1)
   unlist_windows <- unlist(windows)
  at <- ranges(unlist_windows)
   
  k_mers <- extractAt(unlisted, at)

  #OBTAINING THE PLOT: I grouped by sequence, and then grouped by the obtained value and summed to obtain
  #the frequency. 
  k_mers_dt <- as.data.table(k_mers) 

  names(k_mers_dt) <- "sequence"

  k_mers_dt_count <- k_mers_dt %>% group_by(sequence) %>% summarize(value=n()); k_mers_dt_count

  k_mers_dt_count_plt <- k_mers_dt_count %>% group_by(value) %>% summarize(number_of_kmers_per_value = n())


  plt <- k_mers_dt_count_plt %>% ggplot(aes(x=value, y=number_of_kmers_per_value, fill=value))+geom_col()
  
  list <- list(kmers= k_mers, error_fragments= eFrags, reads_without_error=reads, plot=plt)
  return(list)
  
}

result <- my.kmer.coverage(seq, 20, 20)

```

Can you distinguish between kmers that have an error and error-free kmers? If you had a diploid organism, could you distinguish between kmers with SNPs (bases that differ in homologuous chromosomes) and errors? Explain how you would do this and show on an example (get 2 DNA strings as input instead of one, and make them differ in 5% of sites to represent SNPs).  Could you distinguish between kmers with SNPs (bases that differ in homologuous chromosomes) and errors? Explain how you would do this and show on an example (get 2 DNA strings as input instead of one, and make them differ in 5% of sites to represent SNPs).



As we can see in the plot, the K-mers with error are at the beginning because they are much less frequent than error free K-mers. That would be a easy way to distinguish between error/error-free K-mers when assesing the quality of the sequencing. In fact, most of the sequencing error correction tools implement the k-mer counting methods. In k-mer counting methods, low-depth k-mers are assumed to be erroneous. The k-mers resulting from heterozygosity should be more carefully managed because these k-mers are similar to the erroneous k-mers


```{r pressure, echo=FALSE}


#Introducing 5% difference representing the SNPs in order to create a new sequence. 
    seq1 <- seq
   
    snp_pos <- sample(nchar(seq1),nchar(seq1)*0.05, replace=TRUE)
    
    letter_snp <- DNAString( paste(sample(c('A', 'C', 'G', 'T'), length(snp_pos),  replace=TRUE), collapse=""))
    
    snp_letters <- seq1[snp_pos]
    
    cmp <- compareStrings(letter_snp, snp_letters)
    
    positions_A <- unlist(str_locate_all(cmp, "A"))
    
    positions_TGC <- unlist(str_locate_all(cmp, "[TCG]"))
    
    new_let <- letter_snp
    
    new_let[positions_A] <- "G"
    
    new_let[positions_TGC] <- "A"
    
     seq2 <- replaceLetterAt(seq1, snp_pos, new_let)
  
    
    
    
``` 
    
In order to simulate a sequencing experiment with a diploid DNA, I inputed the seq2 (which represents the other dosis of DNA) and calculated the kmers. It is a common problem when asessing the quality of the sequencing process to distinguish between error k-mers and SNP. In this graph, I joined all the kmers together to simulate the reads we could obtain when sequencing the DNA of a heterozygous organism. As we can see, it is really easy to distinguish the error k-mers, (like in the other graph), because they appear in a exponentially decreasing curve. We can also see that the curve of the k-mer density plot follows a normal distribution ideally, and the humps that are not in this peak represent different k-mers. However I find this graph a little bit confusing, as kmers corresponding to SNPs should appear represented as a hump at the left of the main peak, as they are less common. However, the question was if it would be possible to distinguish between the peaks belonging to SNPs and error k-mers, and the answer is that it would be easy to distinguish it graphically, because the slope of the error k-mers is quite characteristic, and the humps that represent SNPs appear more to the right in the x-axis. This difference between error k-mers and SNP k-mers is due to the fact that error k-mers are not present in all the reads, while SNP k-mers are. It is worthy to note however, that there is a problem with this method of k-mer counting becausue there are less frequent k-mers that are treated as error k-mers, and vice-versa (error k-mers can be treated as error free, if they're very frequent. )

```{r}

 
  result2 <- my.kmer.coverage(seq2, 20, 20)

  k_mers2 <- result2[[1]]
  k_mers1 <- result[[1]]
      
  k_dt1 <- as.data.table(kmers1) 
  names(k_dt1) <- "sequence"
  k_dt2 <- as.data.table(kmers2) 
  names(k_dt2) <- "sequence"
  
  
  kmers_sequenced <- rbind(k_dt1, k_dt2)
  k_count <- kmers_sequenced %>% group_by(sequence) %>% summarize(value=n())
  plt <- k_count %>% ggplot()+ geom_density(aes(x=value, color="blue"), show.legend = F)+labs(x="kmer depth", y="Frequency")+ggtitle("K-mer density plot"); plt
  
  
      
      
```
    

 
    
    ```{r}
    
    if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("seqLogo")
    
    ```
    Install the seqLogo package with BiocManager::install("seqLogo"). Find all the reads which contain the sequence "ATCGTCGAGTC" allowing for maximum 5 mismatches, no indels. Make the position weight matrix (Biostrings package function) and plot the logo for all those matches (seqLogo package). Make the logo for all those matches which have 5 mismatches, 4 mismatches, 3 mismatches, 2 mismatches or 1 mismatch. 
    
    ```{r pressure, echo=FALSE}  
    
library(seqLogo)
    
rd <- result[[2]]
rds_unlist <- unlist(rd)
ranges_with_pattern <- matchPattern("ATCGTCGAGTC", rds_unlist, max.mismatch=5, with.indels=FALSE)
patterns_list <- extractAt(rds_unlist, ranges_with_pattern)
wm <- consensusMatrix(patterns_list, as.prob = TRUE)[1:4, ]
pwm <- makePWM(wm, alphabet = "DNA")
plt_logo <- plot(pwm); plt_logo


ranges_with_pattern4 <- matchPattern("ATCGTCGAGTC", rds_unlist, max.mismatch=4, with.indels=FALSE)

patterns_list4 <- extractAt(rds_unlist, ranges_with_pattern4)
wm4 <- consensusMatrix(patterns_list, as.prob = TRUE)[1:4, ]
pwm4 <- makePWM(wm4, alphabet = "DNA")
plt_logo4 <- plot(pwm4); plt_logo4

ranges_with_pattern3 <- matchPattern("ATCGTCGAGTC", rds_unlist, max.mismatch=3, with.indels=FALSE) 

patterns_list3 <- extractAt(rds_unlist, ranges_with_pattern3)
wm3 <- consensusMatrix(patterns_list3, as.prob = TRUE)[1:4, ]
pwm3 <- makePWM(wm3, alphabet = "DNA")
plt_logo3 <- plot(pwm3); plt_logo3


ranges_with_pattern2 <- matchPattern("ATCGTCGAGTC", rds_unlist, max.mismatch=2, with.indels=FALSE)

patterns_list2 <- extractAt(rds_unlist, ranges_with_pattern2)
wm2 <- consensusMatrix(patterns_list2, as.prob = TRUE)[1:4, ]
pwm2 <- makePWM(wm2, alphabet = "DNA")
plt_logo2 <- plot(pwm2); plt_logo2


#When we restrict the mismatches to 1, we can see that there are no matching patterns. 

ranges_with_pattern1 <- matchPattern("ATCGTCGAGTC", rds_unlist, max.mismatch=1, with.indels=FALSE)

patterns_list1 <- extractAt(rds_unlist, ranges_with_pattern1) 


``` 

2. Create a function that does heuristic k-means clustering (name it My.k.means) and plots the final result (final Voronoi regions should be visible). It should also return positions of the centroids and indication which input point belongs to which centroid. You need to do multiple runs in order to find the consensus centroid position. If points are defined in a space with more than 2 dimensions you do not need to do the plotting. Of course, you are not allowed to use any R function which deals with k-means clustering, from any package. The created function must work on any table or matrix. 

    
```{r pressure, echo=FALSE} 
n <- 4
K <- 2
d <- 2
x <- matrix(sample(100, n * d), ncol = n)
x


```
In the context of k-means, we want to partition the space of our observations into k classes. Each observation belongs to the cluster with the nearest mean. Here “nearest” means the one with the lowest Euclidean distance.  What I concluded after my research, is that voronoi diagram divides a space is divided into cells so that each cell contain exactly one site for every point in cell. The Euclidean distance of the point to the site within the cell must be smaller than the distance of that point to may other site in the plane. Therefore, I concluded that in order to represent vonoroi cells in my plot, I need to assign every point of the area I'm representing to a cluster (using my k.means function (nearest point) and represent them by group. 



```{r pressure, echo=FALSE} 

my.k.means <- function(matrix, n_k, n_p) {
  
#My function takes 4 inputs: my matrix of data, the centers (they should be of the same dimensions as the matrix, and the number of centroids is K). The organisation of the data is as follows: the x and y are in the columns, so if the matrix has 2 rows and 50 columns, it means that the points are in 2D and the number of points are 50. The centers should have the same dimensions as the matrix. 
  
  cent <- matrix[,sample(ncol(matrix),n_k)]
  result <- matrix(0,nrow(matrix),n_k)
  result2 <- cent
 
  
#First, I used this loop that calculates the distance of each element of the cluster to the elements in the matrix of points (Euclidean formula)
while(isTRUE(all.equal(result2,result)) == FALSE) {

  dists <- apply(cent, 2, function(center) {
       colSums((matrix - center)^2)
})
dists <- sqrt(dists)

#Then, I named my table of distances : each column represents the distances of each point to one of the clusters. 
n_centers <- ncol(cent)
names_cols <- lapply(seq(1:n_centers), function(i) paste("cluster_",i, sep="" ))
euclidean_distances <- dists
colnames(euclidean_distances) <- names_cols

#Each row represents one point, and they're ordered, so that means that if we retrieve the lowest, we have a way to identify
#which is the closest cluster for each of the points. 

lowest <- colnames(euclidean_distances)[apply(euclidean_distances,1,which.min)]
points_id <- seq(1:ncol(matrix))
corresponding_cluster <- as.data.table(cbind(points_id, lowest))


#Now, we have a list of clusters and the id of the corresponding points, and we can use the position
#to subset in the original matrix and retrieve the points. We do the rowwise means, to do the means
#of the x coordinates and the y coordinates of each point in the cluster. 

points_by_cluster <- split(corresponding_cluster$points_id,corresponding_cluster$lowest)
cluster_list <- lapply(points_by_cluster, function(cluster) { lapply(cluster, function(pos) matrix[, as.numeric(pos)])})
cluster_list_df <- lapply(cluster_list, function(x) as.data.table(x))
clusters_new_centroids <- lapply(cluster_list_df, function(x) rowMeans(x))


#We now have the new clusters and put them in the right format. 

cols <- lapply(clusters_new_centroids, function(x) as.data.table(x))
centroids <- as.matrix(do.call(cbind, cols))
result <- result2
result2 <- centroids

}
  
final <- list(corresponding_cluster, cluster_list, centroids)
return(final)


}


result <- my.k.means(x, K, n)
```


I didn't know whether to put the plot and the while inside of the function, but finally I decided not to do it because my objective is that my function calculates new centroids for a matrix of points in every dimension, and the plot should just be done for a matrix of points of two dimensions, so I think the function makes more sense the way I divided it. With the purpose of running it several times until the consensus centroids are calculated, I implement a while loop outside the function, that returns the final centroids as "centers" and the corresponding final cluster as "corresponding_final"


```{r pressure, echo=FALSE}
dt <- t(x)
centroids_dt <- t(result[[3]])

dt<- as.data.table(dt)
centroids_dt <- as.data.table(centroids_dt)

colnames(dt) <- c("x_axis", "y_axis")
colnames(centroids_dt) <- c("x_axis", "y_axis")

grouped_data <- cbind(dt, corresponding_final[,2]) %>% rename(lowest="Cluster")

plt1 <- ggplot() + geom_point(data=grouped_data, aes(x=x_axis, y=y_axis, color=Cluster), size=5)+geom_point(data=centroids_dt, aes(x=x_axis, y=y_axis), shape=23, fill="blue", color="darkred", size=7)

plt1

```

```{r pressure, echo=FALSE}
#Another plot for vonoroi, because I think it is clearer to see both plots. 
    
x_coord <- x[1,]; x_coord
y_coord <- x[2,]; y_coord


max_coordx <- max(x_coord); max_coordx

max_coordy <- max(y_coord)
x_points <- seq(1:max_coordx)

y_points <- seq(1:max_coordy)

assign_cluster <- function(matrix, cent, n_p) {
dists <- apply(cent, 2, function(center) {
       colSums((matrix - center)^2)
})
dists <- sqrt(dists)
n_centers <- ncol(cent)
names_cols <- lapply(seq(1:n_centers), function(i) paste("cluster_",i, sep="" ))
euclidean_distances <- dists
colnames(euclidean_distances) <- names_cols
lowest <- colnames(euclidean_distances)[apply(euclidean_distances,1,which.min)]
points_id <- seq(1:n_p)
corresponding_cluster <- as.data.table(cbind(points_id, lowest))
final <- corresponding_cluster
return(final)
}

grid <- expand.grid(x_points, y_points)
colnames(grid) <- c("x_axis", "y_axis")

grid_for_cluster <- t(grid)

cluster_grid <- assign_cluster(grid_for_cluster, result[[3]], ncol(grid_for_cluster))



dt_plot <- cbind(grid, cluster_grid)

plt_voronoi <-ggplot() + geom_point(data=dt_plot, aes(x=x_axis, y=y_axis, color=lowest, alpha = 0.2))+labs(color="Voronoi regions")+ geom_point(data=centroids_dt, aes(x=x_axis, y=y_axis), shape=23, fill="blue", color="darkred", size=7)+scale_alpha(guide = 'none')+geom_point(data=grouped_data, aes(x=x_axis, y=y_axis, color=Cluster), size=5)

  
  
plt_voronoi  
  

```




## Exercise 3 easy version: FASTQ analysis  
#### 10 points  
  
You can find 1000 sequences in fastq format in this link https://d28rh4a8wq0iu5.cloudfront.net/ads1/data/ERR037900_1.first1000.fastq.  
If you have any doubts about this exercise, you should have a look at FASTQC - this is a program commonly used to assess the quality of next generation sequencing experiments, similar to what you will be doing in this exercise. You can find the explanations of the graphs it produces here:  https://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/  
   
#### Part 1:  
   
(1) Explain how the fastq file is organised and what information does it hold. (Yes, google. But please explain in your own words. What are the different lines? How many lines per sequence?)  

FASTQ format is a text-based format containing the information of a sequence (usually nucleotide) and its corresponding quality scores obtained by sequencing. It contains the sequence of the reads obtained by sequencing. 

A FASTQ file usually uses four lines per sequence.

First line begins with a  '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line). 

The second line is the raw sequence letters.

Line 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again (not in this case). 

Line 4 represents the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence. In order to understand the format correctly, we have to know that the quality value Q is an integer mapping of p (i.e., the probability that the corresponding base call is incorrect). 
  

The quality values in increasing order of quality are :  !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
  
#### Part 2:  
  
(1) What is Phred quality score? Write the function that will extract quality scores for reads in numeric values (Sanger Phred+33). Plot the mean PER BASE (for every position) quality for reads. (Yes, google. But write your own code that will extract quality scores and plot the mean.)  

A Phred quality score is a measure of the quality of the sequencing for each base. Phred quality scores Q are logarithmically related to the base-calling error probabilities P. So, for example if Phred is 20, the probability of a base being called wrong is 1 in 100, and the base call accuracy (the probability of the base being called correctly) is 99%. The formula that connects Q and P (probability of error base call) is: 


$$
Q =  -10log{P}
$$
```{r pressure, echo=FALSE}

Qscores <- function(stringq) {
    stringq <- as.character(stringq)
    number <- as.vector(sapply(stringq, function(char) as.numeric(charToRaw(char))-33))
    dt <- as.data.table(number)
    dt <- as.data.table(t(dt))
}


fastq <- fread("https://d28rh4a8wq0iu5.cloudfront.net/ads1/data/ERR037900_1.first1000.fastq", header=FALSE)


fastq_score <-fastq[seq(4, nrow(fastq), 4), ]
fastq_ids <-fastq[seq(1, nrow(fastq), 4), ]
  
  
qscore_ids <- cbind( fastq_ids ,fastq_score)
colnames(qscore_ids) <- c("id", "qscore")


matrix <-rbindlist(lapply(qscore_ids$qscore, function(line) Qscores(line)))

matrix_mean <- as.data.table(colMeans(matrix)) %>% mutate(pos = 1:100) %>% rename(V1="mean_score"); matrix_mean


plt <- matrix_mean %>% ggplot(aes(x=pos, y= mean_score,fill=mean_score))+geom_col()+labs(x="Position(pb)", y="Mean Q", color=NULL)+ggtitle("Mean per base of Q scores"); plt

```
  
#### Part 3: 
  
(2) Plot the percentage of A,C,T,G nucleotides for every position (==PER BASE) on a single graph, using lines of different colors. (see example_graph1 - it doesn't necessarily need to be colored or formatted this way, this is just an example)    

```{r pressure, echo=FALSE}

sequences = readDNAStringSet("https://d28rh4a8wq0iu5.cloudfront.net/ads1/data/ERR037900_1.first1000.fastq", format="fastq")

consensus_matrix <-  consensusMatrix(sequences, as.prob = TRUE)[1:4,]

consensus_matrix_dt <- as.data.table(t(consensus_matrix))


consensus_matrix_dt <- consensus_matrix_dt %>% mutate_all(., ~ . * 100) %>% mutate(position_in_read = seq(1:n()))


consensus_matrix_perc <- melt(consensus_matrix_dt, id="position_in_read", measure=c("A", "C", "G", "T"))

plt <- consensus_matrix_perc  %>% ggplot(aes(x=position_in_read, y=value, color=variable))+ geom_line()+labs(x="Position (pb)", y="Percentage of bases", color="Bases")+ggtitle("Percentage of base per position")

plt

```
#### Part 4:  
  
(1) For each base, plot a boxplot of quality across all reads. Do this for all bases. (see example_graph2 - it doesn’t need to be colored or formatted this way, this is just an example)  
```{r pressure, echo=FALSE}

matrix_boxplot <- as.data.table(t(matrix)) %>% mutate(pos=seq(1:100))

matrix_boxplot <- melt(matrix_boxplot, id="pos")

 
matrix_boxplot <-  matrix_boxplot %>% mutate(across(pos, as.factor))

box_plt <- matrix_boxplot %>% ggplot(aes(x=pos, y=value, color=pos))+geom_boxplot(outlier.shape = NA, show.legend = F)+labs(x="Position (pb)", y="")+ggtitle("Quality scores across all bases")+scale_x_discrete(breaks = c(1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100))

box_plt

```
#### Part 5:  
  
(2) For each position, calculate and plot the estimated probability of a base being wrong. If you disregard the strange phenomenon observed in question 2, how does quality depend on the position in the read? Why does this happen? Explain having in mind the sequencing procedure - assume this was sequenced by Illumina technology. 


As we said before, this probability is calculated with the formula mentioned above. 

The errors are more prone to ocurr at the end of the reads. Previous studies have shown that Illumina errors are not random and that there is an increase of errors towards the end of the reads.  For sequencing-by-synthesis methods, such as Illumina, the DNA polymerase is a key element. The polymerase used in this type of sequencing permits the incorporation of chain termination dideoxynucleotides (ddNTPs) which inhibit the DNA synthesis, and these are another key element for this sequencing method.


 The chemical and structural properties of ddNTPs contribute to the effect of increased errors by the end. After the cleavage of the linker group carrying the fluoresphore, extra chemical molecules on the normal purine and pyrimidine bases remain and result in a vestige. These vestiges can perturb the DNA polymerase and limited the possible read length as they impair the stability of the DNA and hinders the substrate recognition and primer extension. Studies have already talked about an accumulation of these vestiges in Illumina sequencing. Illumina has been able to achieve longer reads by adding reversible terminator nucleotides without the fluorophore to reduce the effect of vestiges, but their impact is still apparent as increased error rates towards the end of the reads.



```{r pressure, echo=FALSE}

prob_per_position <- matrix_mean %>% mutate(probability=(10)^(-mean_score/10)) 
plt <- prob_per_position %>% ggplot(aes(x=pos, y=probability, color="orange"))+geom_line(show.legend = F)+ggtitle("Probability of error per position (Illumina sequencing)")+labs(x="Position (bp)", y="Probability of error")

plt


```
  
#### Part 6:  
  
(1) Plot per sequence quality, like in the example graph.
```{r pressure, echo=FALSE}

sequence_quality <- matrix_boxplot %>% group_by(variable) %>% summarize(q_read=mean(value))

plt <- sequence_quality_plot %>% ggplot(aes(x=q_read, color="red"))+geom_freqpoly(show.legend = F)+labs(x="Mean Sequence Quality", y="")+ggtitle("Quality score distribution over all reads")

plt
```  
(2) Plot per sequence GC content distribution along with theoretical distribution, like in the example graph.
  
```{r pressure, echo=FALSE}  
  
reads_fastq <-fastq$V1[seq(2, nrow(fastq), 4)]

reads_fastq <- DNAStringSet(reads_fastq)

GC_freq <- as.data.table(letterFrequency(reads_fastq, "GC")) ; GC_freq

colnames(GC_freq) = "gc"


sd <- sd(GC_freq$gc)
mean <- mean(GC_freq$gc)

theoretical <- rnorm(1000, mean, sd)

GC_freq <- as.data.table(cbind(GC_freq, theoretical))

plt <- GC_freq %>% ggplot()+geom_density(aes(x=gc), color="red", show.legend = F)+geom_density(aes(x=theoretical), color="blue")+labs(x="GC")+ggtitle("GC content distribution")

plt

```


